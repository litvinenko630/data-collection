{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c301448",
   "metadata": {},
   "source": [
    "## Парсер hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23fa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт класса BeautifulSoup из библиотеки bs4\n",
    "from bs4 import BeautifulSoup \n",
    "# Импорт модуля requests для отправки HTTP-запросов\n",
    "import requests  \n",
    "# Импорт библиотеки numpy для работы с массивами и числами\n",
    "import numpy as np  \n",
    "# Импорт библиотеки pandas для работы с данными в виде таблиц\n",
    "import pandas as pd  \n",
    "# Импорт модуля re для работы с регулярными выражениями\n",
    "import re  \n",
    "# Импорт модуля warnings для управления предупреждениями\n",
    "\n",
    "import json\n",
    "import warnings  \n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings('ignore')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ecaa1",
   "metadata": {},
   "source": [
    "### Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность) с сайта HH. Приложение должно анализировать все страницы сайта.\n",
    "\n",
    "**Получившийся список должен содержать в себе минимум:**\n",
    "1. Наименование вакансии.\n",
    "2. Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).\n",
    "3. Ссылку на саму вакансию.\n",
    "4. Сайт, откуда собрана вакансия.\n",
    "5. По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение).\n",
    "6. Общий результат можно вывести с помощью dataFrame через pandas. Сохраните в json либо csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8872631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите интересующую должность: Аналитик\n",
      "Обработана страница №0\n",
      "Обработана страница №1\n",
      "Обработана страница №2\n",
      "Обработана страница №3\n",
      "Обработана страница №4\n",
      "Обработана страница №5\n",
      "Обработана страница №6\n",
      "Обработана страница №7\n",
      "Обработана страница №8\n",
      "Обработана страница №9\n",
      "Обработана страница №10\n",
      "Обработана страница №11\n",
      "Обработана страница №12\n",
      "Обработана страница №13\n",
      "Обработана страница №14\n",
      "Обработана страница №15\n",
      "Обработана страница №16\n",
      "Обработана страница №17\n",
      "Обработана страница №18\n",
      "Обработана страница №19\n",
      "Обработана страница №20\n",
      "Обработана страница №21\n",
      "Обработана страница №22\n",
      "Обработана страница №23\n",
      "Обработана страница №24\n",
      "Обработана страница №25\n",
      "Обработана страница №26\n",
      "Обработана страница №27\n",
      "Обработана страница №28\n",
      "Обработана страница №29\n",
      "Обработана страница №30\n",
      "Обработана страница №31\n",
      "Обработана страница №32\n",
      "Обработана страница №33\n",
      "Обработана страница №34\n",
      "Обработана страница №35\n",
      "Обработана страница №36\n",
      "Обработана страница №37\n",
      "Обработана страница №38\n",
      "Обработана страница №39\n",
      "Этот этап завершен\n"
     ]
    }
   ],
   "source": [
    "# Определяем функцию для обработаки вида валюты\n",
    "def curency(el):\n",
    "    if el == '₽':\n",
    "        jobs_info['currency'] = 'RUB'\n",
    "    elif el == '$':\n",
    "        jobs_info['currency'] = 'USD'\n",
    "    elif el == '€':\n",
    "        jobs_info['currency'] = 'EUR'\n",
    "    else:\n",
    "        jobs_info['currency'] = el\n",
    "    return jobs_info['currency']\n",
    "\n",
    "\n",
    "# Определяем переменную для хранения данных\n",
    "PREPARED_FILE_PATH = 'hh_parsed.csv'\n",
    "PREPARED_JSON_FILE_PATH = 'vacancies.json'\n",
    "\n",
    "# Заголовки для HTTP-запросов, чтобы представлять себя как обычный веб-браузер\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\\\n",
    "    Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Ввод интересующей должности\n",
    "desired_job = input('Введите интересующую должность: ')\n",
    "\n",
    "\n",
    "# Параметры запроса\n",
    "params = {\n",
    "    'text': desired_job,\n",
    "    'page': 0, # Стартовая страница\n",
    "    'area': 1, # 'false' если поиск необходимо осуществить по всей Россиии (Москва - 1)\n",
    "    'search_period': 7 # Период, за который выводятся значения\n",
    "}\n",
    "\n",
    "# URL для запроса\n",
    "url = \"https://hh.ru\"\n",
    "\n",
    "# Строка поиска\n",
    "search_line = '/search/vacancy?'\n",
    "\n",
    "# Создание сессии для отправки запросов\n",
    "session = requests.Session()\n",
    "\n",
    "# Список для хранения информации о вакансиях\n",
    "job_list = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Отправка запроса на страницу поиска\n",
    "    response = session.get(url + search_line, headers=headers, params=params)\n",
    "    \n",
    "    # Если вакансий нет, прерываем цикл\n",
    "    if not response.ok:\n",
    "        break\n",
    "        \n",
    "    # Создание объекта BeautifulSoup для парсинга HTML-кода\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Поиск всех элементов с классом 'serp-item' (вакансии)\n",
    "    jobs = soup.find_all('div', {'class': 'serp-item'})\n",
    "\n",
    "    # Обход каждой вакансии\n",
    "    for job in jobs:\n",
    "        # Создание словаря для хранения информации о вакансии\n",
    "        jobs_info = {}\n",
    "\n",
    "        # Извлечение названия и ссылки на вакансию\n",
    "        info = job.find('a', {'class': 'serp-item__title'})\n",
    "        name = info.text\n",
    "        link = info.get('href')\n",
    "\n",
    "        # Извлечение идентификатора вакансии\n",
    "        start_index = link.find(\"vacancy/\") + len(\"vacancy/\")\n",
    "        end_index = link.find(\"?\")\n",
    "        vacancy_id = link[start_index:end_index]\n",
    "\n",
    "        # Извлечение местоположения вакансии\n",
    "        location = job.find('div', {'data-qa': \"vacancy-serp__vacancy-address\"}).text\n",
    "        \n",
    "        # Извлечение требуемого опыта работы\n",
    "        experience = job.find('div', {'data-qa': 'vacancy-serp__vacancy-work-experience'}).text\n",
    "        \n",
    "\n",
    "        # Извлечение компании вакансии\n",
    "        employer_info = job.find('a', {'data-qa': \"vacancy-serp__vacancy-employer\"})\n",
    "        \n",
    "        try:\n",
    "            # Проверяем наличие информации о компании\n",
    "            if employer_info is None:\n",
    "                # Если информации нет, устанавливаем значение работодателя как NaN\n",
    "                employer = np.nan\n",
    "            else:\n",
    "                # Если информация о компании присутствует, извлекаем текст (имя компании) и ссылку на \n",
    "                # страницу компании\n",
    "                employer = employer_info.text\n",
    "                # Извлечение ссылки на страницу компании\n",
    "                employer_page = employer_info.get('href')\n",
    "                employer_page = url + employer_page\n",
    "        except Exception as e:\n",
    "            # Обрабатываем возможные ошибки\n",
    "            print(f\"В данном блоке ошибка: {e}\")\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Добавление информации о вакансии в список\n",
    "        source_site = url\n",
    "        jobs_info['job_title'] = name\n",
    "        jobs_info['experience'] = experience\n",
    "        jobs_info['location'] = location\n",
    "        jobs_info['vacancy_id'] = vacancy_id\n",
    "        jobs_info['source_site'] = source_site\n",
    "        jobs_info['link'] = link\n",
    "        \n",
    "        salary = job.find(\"span\", {\"data-qa\": \"vacancy-serp__vacancy-compensation\"})\n",
    "        try:\n",
    "            # Проверяем наличие информации о зарплате\n",
    "            if salary == None:\n",
    "                # Если информации нет, устанавливаем значения зарплаты и валюты как NaN\n",
    "                jobs_info['min_salary'] = np.nan\n",
    "                jobs_info['max_salary'] = np.nan\n",
    "                jobs_info['currency'] = np.nan\n",
    "            else:\n",
    "                # Если информация о зарплате есть, обрабатываем ее\n",
    "                salary_info = salary.text.replace('\\u202f', '').split()\n",
    "                if any('от' in el for el in salary_info) and any('до' in el for el in salary_info):\n",
    "                    # Зарплата указана в диапазоне (от ... до ...)\n",
    "                    jobs_info['min_salary'] = int(salary_info[1])\n",
    "                    jobs_info['max_salary'] = int(salary_info[3])\n",
    "                    curency(salary_info[-1])\n",
    "\n",
    "                elif any('до' in el for el in salary_info):\n",
    "                    # Зарплата указана максимальной (до ...)\n",
    "                    jobs_info['min_salary'] = np.nan\n",
    "                    jobs_info['max_salary'] = int(salary_info[1])\n",
    "                    curency(salary_info[-1])\n",
    "\n",
    "                elif any('от' in el for el in salary_info):\n",
    "                    # Зарплата указана минимальной (от ...)\n",
    "                    jobs_info['min_salary'] = int(salary_info[1])\n",
    "                    jobs_info['max_salary'] = np.nan\n",
    "                    curency(salary_info[-1])\n",
    "\n",
    "                else:\n",
    "                    # Зарплата указана точно (от ...  - до ...)\n",
    "                    jobs_info['min_salary'] = int(salary_info[0])\n",
    "                    jobs_info['max_salary'] = int(salary_info[2])\n",
    "                    curency(salary_info[-1])\n",
    "        except Exception as e:\n",
    "            # Обрабатываем возможные ошибки\n",
    "            print(f\"В данном блоке ошибка: {e}\")\n",
    "            pass\n",
    "\n",
    "        \n",
    "        jobs_info['employer'] = employer\n",
    "        jobs_info['employer_page'] = employer_page\n",
    "        \n",
    "        job_list.append(jobs_info)\n",
    "\n",
    "    # Вывод номера обработанной страницы\n",
    "    print(f\"Обработана страница №{params['page']}\")\n",
    "\n",
    "    # Увеличение значения параметра 'page' для перехода на следующую страницу\n",
    "    params['page'] += 1\n",
    "\n",
    "# Вывод сообщения об окончании цикла обработки\n",
    "print('Этот этап завершен')\n",
    "\n",
    "# Создание DataFrame для списка с информацией о должностях\n",
    "df = pd.DataFrame(job_list)\n",
    "\n",
    "# Сохранение DataFrame в csv файл\n",
    "df.to_csv(PREPARED_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0a34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vacancies to a JSON file\n",
    "with open(PREPARED_JSON_FILE_PATH, 'w') as file:\n",
    "    json.dump(job_list, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
